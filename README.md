# Distributed Hash Table - PPCA 2023

使用Chord算法和go语言实现了分布式哈希表。

## Chord

所有的节点组成一个环，对网络地址取sha1算法得到在环上的位置。使用Finger Table记录当前位置$2^k$后位置的后继节点，利用了倍增的思想加速环上元素的查找。

### 代码结构

`chord.go` rpc服务和chord中维护环结构相关的函数

`chordClient.go` rpc客户端的封装

`chordServer.go` rpc服务端的封装，应答chord协议中的远程请求

`node.go` 实现dhtNode接口

### 算法细节补充1（环结构部分）

- 在节点正常退出时可以通知前驱连接自己的后继和通知后继连接自己的前驱，以此快速维持环的结构。

- 为了应对Force Quit的情况，为了让其前驱节点尽可能得到准确的后继，维护一定长度的后继列表，每次需要后继时从中取出第一个能成功连接的。不需维护多个前驱的原因是：只要保证后继尽可能准确，在stabilize中调用notify时就可以使得后继的前驱正确。

- 分清允许向自己请求/连接的地方和不允许的地方。

### 算法细节补充2（维护数据部分）

- 每个节点备份其前驱的数据，当前驱失效时，其数据应当由后继负责，此时后继直接启用备用数据，之后被notify时更新备份数据，用这种方法可以减少数据传输。但如果相邻的几个节点同时Force Quit，就会引起数据丢失。（测试点中没有这种情况）

- 所有的数据被更改时，应当同步后继中备份数据的更改。

### debug和设计方法

- 大部分bug发现流程：通过log异常表现猜测错误出现处（比如Join后Get错误），然后构造小数据点（见`chord_test.go`）尝试复现bug，然后在敏感的操作处加log输出观察结果。

- 尽可能设计顺序、同层的操作而不是递归的操作；尽可能使得每一段程序能应对所有可能的情况，增强鲁棒性；在所有的get调用中都不应该对被调用者数据有更改（类似于c中的const方法），使得数据在尽可能少的地方被更改，更容易判断数据在哪个流程中出现问题。
## Application 

[D-Torrent](https://github.com/s7a9/D-Torrent) （230713完成约80%）

- 把每一个文件分割为许多piece，对每一个piece取sha1的base64编码作为标识符。

- dht表中，以piece的hash标识符作为键值，拥有piece的peer地址列表作为值。另外实现文件传输服务以传输piece。

- 要下载文件，先获得种子文件，种子文件内保存了该文件所有piece的hash标识符，然后对于每个piece在dht中查找在线的服务器列表，随机挑选在线的服务器下载，并删除不在线的服务器。

- 要做种，检查文件每个piece的hash是否和种子文件的相符，若相符则把自己的地址放在dht中。